{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Warm_up.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Walp6l-dW4sHZyHc56rGcIpmUpIpfeP-",
      "authorship_tag": "ABX9TyNo+WjB1RmLT8SxxM5iz9Xn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammalik221/Mini-Projects/blob/master/PyTorch_Warm_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDA7gEKP-TJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80e33450-1b8b-4847-cfef-96a4d324ce54"
      },
      "source": [
        "######################################################\n",
        "################# Basic Operations ###################\n",
        "######################################################\n",
        "\n",
        "import torch\n",
        "\n",
        "\"\"\" Torch tensors are similar to numpy's ndarray, the difference is just \n",
        "that, they can be easily transferred to GPU. Any operation that we want \n",
        "to perform on ndarray can also be performed on tensors.\n",
        "\"\"\"\n",
        "\n",
        "# a 2x3 tensor\n",
        "n_tensor = torch.Tensor([[1,2,3],\n",
        "                         [5,6,7]])\n",
        "print(\"New Tensor - \\n\", n_tensor,  \"\\n\")\n",
        "\n",
        "# a 2x3 tensor of zeros\n",
        "z_tensor = torch.zeros(2, 3)\n",
        "print(\"Tensor with all zeros - \\n\", z_tensor, \"\\n\")\n",
        "\n",
        "# a 2x3 tensor of random values in the range [0, 1)\n",
        "r_tensor = torch.rand(2, 3)\n",
        "print(\"Tensor with random value - \\n\", r_tensor, \"\\n\")\n",
        "\n",
        "# accessing a scalar object\n",
        "s_tensor = torch.Tensor([[1,2,3],\n",
        "                         [5,6,7]])\n",
        "print(\"Element at a position - \\n\", s_tensor[0][1].item(), \"\\n\")\n",
        "\n",
        "# slicing\n",
        "slice_tensor = torch.Tensor([[1,2,3],\n",
        "                             [5,6,7]])\n",
        "print(\"All rows, first column\\n\", slice_tensor[:, 0], \"\\n\")\n",
        "print(\"All rows, last column\\n\", slice_tensor[:, -1], \"\\n\")\n",
        "print(\"All columns, first row\\n\", slice_tensor[0, :], \"\\n\")\n",
        "print(\"All columns, last column\\n\", slice_tensor[-1, :], \"\\n\")\n",
        "\n",
        "# info of tensor\n",
        "i_tensor = torch.Tensor([[1,2,3],\n",
        "                         [5,6,7],\n",
        "                         [8,9,10]])\n",
        "print(\"Type - \\n\", i_tensor.type(), \"\\n\")\n",
        "print(\"Shape - \\n\", i_tensor.shape, \"\\n\")\n",
        "print(\"Dimensions - \\n\", i_tensor.dim(), \"\\n\")\n",
        "\n",
        "# reshape\n",
        "re_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
        "print(\"Tensor before reshaping - \\n\", re_tensor, \"\\n\")\n",
        "\n",
        "re_tensor.view(1, 4)\n",
        "print(\"Tensor after reshaping - \\n\", re_tensor, \"\\n\")\n",
        "\n",
        "# to and from numpy\n",
        "nm_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
        "print(\"Type before changing tensor to array - \", type(nm_tensor), \"\\n\")\n",
        "nm_array = nm_tensor.numpy()\n",
        "print(\"Type before changing tensor to array - \", type(nm_array), \"\\n\")\n",
        "\n",
        "# tensor operations\n",
        "a = torch.randn(3, 3)\n",
        "b = torch.randn(3, 3)\n",
        "\n",
        "print(\"Tensor A - \\n\", a, \"\\n\")\n",
        "print(\"Tensor B - \\n\", b, \"\\n\")\n",
        "print(\"Transpose\\n\", a.t(), \"\\n\")\n",
        "print(\"Matrix Multiplication\\n\", a.mm(b), \"\\n\")\n",
        "print(\"Element-wise multiplication\\n\", a.mul(b), \"\\n\")\n",
        "\n",
        "# cuda related operations\n",
        "print(\"True if GPU is available\\n\", torch.cuda.is_available(), \"\\n\")\n",
        "\n",
        "# transfer tensor to CPU\n",
        "a.cpu()\n",
        "\n",
        "# transfer tensor to GPU\n",
        "a.cuda()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New Tensor - \n",
            " tensor([[1., 2., 3.],\n",
            "        [5., 6., 7.]]) \n",
            "\n",
            "Tensor with all zeros - \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]]) \n",
            "\n",
            "Tensor with random value - \n",
            " tensor([[0.4279, 0.1944, 0.3696],\n",
            "        [0.7924, 0.3070, 0.2803]]) \n",
            "\n",
            "Element at a position - \n",
            " 2.0 \n",
            "\n",
            "All rows, first column\n",
            " tensor([1., 5.]) \n",
            "\n",
            "All rows, last column\n",
            " tensor([3., 7.]) \n",
            "\n",
            "All columns, first row\n",
            " tensor([1., 2., 3.]) \n",
            "\n",
            "All columns, last column\n",
            " tensor([5., 6., 7.]) \n",
            "\n",
            "Type - \n",
            " torch.FloatTensor \n",
            "\n",
            "Shape - \n",
            " torch.Size([3, 3]) \n",
            "\n",
            "Dimensions - \n",
            " 2 \n",
            "\n",
            "Tensor before reshaping - \n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]]) \n",
            "\n",
            "Tensor after reshaping - \n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]]) \n",
            "\n",
            "Type before changing tensor to array -  <class 'torch.Tensor'> \n",
            "\n",
            "Type before changing tensor to array -  <class 'numpy.ndarray'> \n",
            "\n",
            "Tensor A - \n",
            " tensor([[ 0.4958,  0.1476, -0.1507],\n",
            "        [ 0.0566,  2.5377,  0.4458],\n",
            "        [ 2.0285, -0.3072,  0.3294]]) \n",
            "\n",
            "Tensor B - \n",
            " tensor([[ 0.6230, -0.8059, -0.4829],\n",
            "        [-0.3902,  0.2282, -2.2761],\n",
            "        [ 1.3331, -0.0927,  1.4708]]) \n",
            "\n",
            "Transpose\n",
            " tensor([[ 0.4958,  0.0566,  2.0285],\n",
            "        [ 0.1476,  2.5377, -0.3072],\n",
            "        [-0.1507,  0.4458,  0.3294]]) \n",
            "\n",
            "Matrix Multiplication\n",
            " tensor([[ 0.0504, -0.3519, -0.7971],\n",
            "        [-0.3606,  0.4922, -5.1477],\n",
            "        [ 1.8228, -1.7354,  0.2042]]) \n",
            "\n",
            "Element-wise multiplication\n",
            " tensor([[ 0.3089, -0.1190,  0.0728],\n",
            "        [-0.0221,  0.5791, -1.0146],\n",
            "        [ 2.7042,  0.0285,  0.4845]]) \n",
            "\n",
            "True if GPU is available\n",
            " True \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4958,  0.1476, -0.1507],\n",
              "        [ 0.0566,  2.5377,  0.4458],\n",
              "        [ 2.0285, -0.3072,  0.3294]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkzTbzTQfEr2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fc47ae6-e339-44d6-d1b2-ac32704568dd"
      },
      "source": [
        "######################################################\n",
        "######## A two layer network with Autograd ###########\n",
        "######################################################\n",
        "\n",
        "\"\"\"\n",
        "Autograd is basically automatic differentiation, if we want to compute\n",
        "gradients of a tensor, we set requires_grad = True. Setting this parameter\n",
        "to true allows us to perform backpropogation on it and the gradient of that\n",
        "tensor is stored in another tensor named tensor_name.grad.\n",
        "\n",
        "The two layer network dimensions are - \n",
        "    input - 1000\n",
        "    hidden - 100\n",
        "    output - 10\n",
        "\"\"\"\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# define necessary variables\n",
        "batch_size = 64\n",
        "input_size = 1000\n",
        "hidden_size = 100\n",
        "output_size = 10\n",
        "\n",
        "# random valued tensors for input and output\n",
        "x = torch.randn(batch_size, input_size, device = device)\n",
        "y = torch.randn(batch_size, output_size, device = device)\n",
        "\n",
        "# initially weights are initialised as random\n",
        "# since we want to perform backpropogation on the weights, we set requires_grad=True\n",
        "w1 = torch.randn(input_size, hidden_size, requires_grad=True, device = device)\n",
        "w2 = torch.randn(hidden_size, output_size, requires_grad=True, device = device)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "epochs = 500\n",
        "for e in range(epochs):\n",
        "\n",
        "    # calculate output of our model\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    # computing and printing loss\n",
        "    loss = ((y_pred - y).pow(2).sum())\n",
        "    print(\"Epoch : \", e,\"\\tLoss - \", loss.item())\n",
        "\n",
        "    # .backward() calcualtes gradient of loss w.r.t all Tensores with \n",
        "    # requires_grad = True\n",
        "    loss.backward()\n",
        "\n",
        "    # since we don't want to build a computational graph for these steps\n",
        "    # we use no_grad() function\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        # zero out the gradients after each epoch, otherwise they\n",
        "        # continue to pile up, for all epochs\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  0 \tLoss -  32229532.0\n",
            "Epoch :  1 \tLoss -  29400756.0\n",
            "Epoch :  2 \tLoss -  30142316.0\n",
            "Epoch :  3 \tLoss -  29423168.0\n",
            "Epoch :  4 \tLoss -  24869062.0\n",
            "Epoch :  5 \tLoss -  17342818.0\n",
            "Epoch :  6 \tLoss -  10319248.0\n",
            "Epoch :  7 \tLoss -  5627142.0\n",
            "Epoch :  8 \tLoss -  3123181.0\n",
            "Epoch :  9 \tLoss -  1886224.0\n",
            "Epoch :  10 \tLoss -  1271321.0\n",
            "Epoch :  11 \tLoss -  939996.375\n",
            "Epoch :  12 \tLoss -  740205.625\n",
            "Epoch :  13 \tLoss -  605383.875\n",
            "Epoch :  14 \tLoss -  506331.5625\n",
            "Epoch :  15 \tLoss -  429138.15625\n",
            "Epoch :  16 \tLoss -  366882.25\n",
            "Epoch :  17 \tLoss -  315726.875\n",
            "Epoch :  18 \tLoss -  273048.3125\n",
            "Epoch :  19 \tLoss -  237109.0625\n",
            "Epoch :  20 \tLoss -  206660.90625\n",
            "Epoch :  21 \tLoss -  180720.34375\n",
            "Epoch :  22 \tLoss -  158505.3125\n",
            "Epoch :  23 \tLoss -  139414.15625\n",
            "Epoch :  24 \tLoss -  122948.53125\n",
            "Epoch :  25 \tLoss -  108699.6015625\n",
            "Epoch :  26 \tLoss -  96323.359375\n",
            "Epoch :  27 \tLoss -  85548.8671875\n",
            "Epoch :  28 \tLoss -  76137.15625\n",
            "Epoch :  29 \tLoss -  67909.578125\n",
            "Epoch :  30 \tLoss -  60692.8671875\n",
            "Epoch :  31 \tLoss -  54333.71875\n",
            "Epoch :  32 \tLoss -  48725.3203125\n",
            "Epoch :  33 \tLoss -  43763.8203125\n",
            "Epoch :  34 \tLoss -  39364.140625\n",
            "Epoch :  35 \tLoss -  35459.28515625\n",
            "Epoch :  36 \tLoss -  31982.251953125\n",
            "Epoch :  37 \tLoss -  28882.58203125\n",
            "Epoch :  38 \tLoss -  26114.89453125\n",
            "Epoch :  39 \tLoss -  23639.0234375\n",
            "Epoch :  40 \tLoss -  21422.23828125\n",
            "Epoch :  41 \tLoss -  19433.21875\n",
            "Epoch :  42 \tLoss -  17646.826171875\n",
            "Epoch :  43 \tLoss -  16040.5615234375\n",
            "Epoch :  44 \tLoss -  14593.736328125\n",
            "Epoch :  45 \tLoss -  13289.3203125\n",
            "Epoch :  46 \tLoss -  12112.7099609375\n",
            "Epoch :  47 \tLoss -  11048.9541015625\n",
            "Epoch :  48 \tLoss -  10086.736328125\n",
            "Epoch :  49 \tLoss -  9216.0869140625\n",
            "Epoch :  50 \tLoss -  8426.998046875\n",
            "Epoch :  51 \tLoss -  7711.7060546875\n",
            "Epoch :  52 \tLoss -  7062.07568359375\n",
            "Epoch :  53 \tLoss -  6471.73095703125\n",
            "Epoch :  54 \tLoss -  5934.625\n",
            "Epoch :  55 \tLoss -  5445.5234375\n",
            "Epoch :  56 \tLoss -  4999.86083984375\n",
            "Epoch :  57 \tLoss -  4593.51611328125\n",
            "Epoch :  58 \tLoss -  4222.7705078125\n",
            "Epoch :  59 \tLoss -  3884.296875\n",
            "Epoch :  60 \tLoss -  3574.975830078125\n",
            "Epoch :  61 \tLoss -  3291.96240234375\n",
            "Epoch :  62 \tLoss -  3033.111328125\n",
            "Epoch :  63 \tLoss -  2796.052978515625\n",
            "Epoch :  64 \tLoss -  2578.85302734375\n",
            "Epoch :  65 \tLoss -  2379.730712890625\n",
            "Epoch :  66 \tLoss -  2197.03955078125\n",
            "Epoch :  67 \tLoss -  2029.361083984375\n",
            "Epoch :  68 \tLoss -  1875.3724365234375\n",
            "Epoch :  69 \tLoss -  1733.811767578125\n",
            "Epoch :  70 \tLoss -  1603.651611328125\n",
            "Epoch :  71 \tLoss -  1483.8677978515625\n",
            "Epoch :  72 \tLoss -  1373.6259765625\n",
            "Epoch :  73 \tLoss -  1272.08203125\n",
            "Epoch :  74 \tLoss -  1178.522705078125\n",
            "Epoch :  75 \tLoss -  1092.27734375\n",
            "Epoch :  76 \tLoss -  1012.728271484375\n",
            "Epoch :  77 \tLoss -  939.3358154296875\n",
            "Epoch :  78 \tLoss -  871.571044921875\n",
            "Epoch :  79 \tLoss -  808.983154296875\n",
            "Epoch :  80 \tLoss -  751.150634765625\n",
            "Epoch :  81 \tLoss -  697.7066650390625\n",
            "Epoch :  82 \tLoss -  648.2745361328125\n",
            "Epoch :  83 \tLoss -  602.558349609375\n",
            "Epoch :  84 \tLoss -  560.2376098632812\n",
            "Epoch :  85 \tLoss -  521.0553588867188\n",
            "Epoch :  86 \tLoss -  484.77423095703125\n",
            "Epoch :  87 \tLoss -  451.1536865234375\n",
            "Epoch :  88 \tLoss -  419.994140625\n",
            "Epoch :  89 \tLoss -  391.1003112792969\n",
            "Epoch :  90 \tLoss -  364.3018798828125\n",
            "Epoch :  91 \tLoss -  339.435546875\n",
            "Epoch :  92 \tLoss -  316.3514404296875\n",
            "Epoch :  93 \tLoss -  294.91796875\n",
            "Epoch :  94 \tLoss -  275.01275634765625\n",
            "Epoch :  95 \tLoss -  256.52227783203125\n",
            "Epoch :  96 \tLoss -  239.33831787109375\n",
            "Epoch :  97 \tLoss -  223.3575439453125\n",
            "Epoch :  98 \tLoss -  208.498291015625\n",
            "Epoch :  99 \tLoss -  194.6757049560547\n",
            "Epoch :  100 \tLoss -  181.8420867919922\n",
            "Epoch :  101 \tLoss -  169.9018096923828\n",
            "Epoch :  102 \tLoss -  158.78521728515625\n",
            "Epoch :  103 \tLoss -  148.43084716796875\n",
            "Epoch :  104 \tLoss -  138.78750610351562\n",
            "Epoch :  105 \tLoss -  129.80238342285156\n",
            "Epoch :  106 \tLoss -  121.42526245117188\n",
            "Epoch :  107 \tLoss -  113.6118392944336\n",
            "Epoch :  108 \tLoss -  106.32579040527344\n",
            "Epoch :  109 \tLoss -  99.52790832519531\n",
            "Epoch :  110 \tLoss -  93.18449401855469\n",
            "Epoch :  111 \tLoss -  87.26148986816406\n",
            "Epoch :  112 \tLoss -  81.73269653320312\n",
            "Epoch :  113 \tLoss -  76.5687255859375\n",
            "Epoch :  114 \tLoss -  71.74513244628906\n",
            "Epoch :  115 \tLoss -  67.238037109375\n",
            "Epoch :  116 \tLoss -  63.026878356933594\n",
            "Epoch :  117 \tLoss -  59.09003448486328\n",
            "Epoch :  118 \tLoss -  55.40766906738281\n",
            "Epoch :  119 \tLoss -  51.965538024902344\n",
            "Epoch :  120 \tLoss -  48.74562072753906\n",
            "Epoch :  121 \tLoss -  45.73192596435547\n",
            "Epoch :  122 \tLoss -  42.913597106933594\n",
            "Epoch :  123 \tLoss -  40.275733947753906\n",
            "Epoch :  124 \tLoss -  37.805076599121094\n",
            "Epoch :  125 \tLoss -  35.492584228515625\n",
            "Epoch :  126 \tLoss -  33.32596969604492\n",
            "Epoch :  127 \tLoss -  31.297618865966797\n",
            "Epoch :  128 \tLoss -  29.397079467773438\n",
            "Epoch :  129 \tLoss -  27.615934371948242\n",
            "Epoch :  130 \tLoss -  25.946645736694336\n",
            "Epoch :  131 \tLoss -  24.38168716430664\n",
            "Epoch :  132 \tLoss -  22.91490364074707\n",
            "Epoch :  133 \tLoss -  21.540042877197266\n",
            "Epoch :  134 \tLoss -  20.249670028686523\n",
            "Epoch :  135 \tLoss -  19.03986930847168\n",
            "Epoch :  136 \tLoss -  17.90443229675293\n",
            "Epoch :  137 \tLoss -  16.83913803100586\n",
            "Epoch :  138 \tLoss -  15.83932876586914\n",
            "Epoch :  139 \tLoss -  14.900859832763672\n",
            "Epoch :  140 \tLoss -  14.020218849182129\n",
            "Epoch :  141 \tLoss -  13.192792892456055\n",
            "Epoch :  142 \tLoss -  12.416133880615234\n",
            "Epoch :  143 \tLoss -  11.686976432800293\n",
            "Epoch :  144 \tLoss -  11.001537322998047\n",
            "Epoch :  145 \tLoss -  10.358064651489258\n",
            "Epoch :  146 \tLoss -  9.753170013427734\n",
            "Epoch :  147 \tLoss -  9.185132026672363\n",
            "Epoch :  148 \tLoss -  8.651203155517578\n",
            "Epoch :  149 \tLoss -  8.149145126342773\n",
            "Epoch :  150 \tLoss -  7.677258014678955\n",
            "Epoch :  151 \tLoss -  7.2331695556640625\n",
            "Epoch :  152 \tLoss -  6.815963268280029\n",
            "Epoch :  153 \tLoss -  6.423133850097656\n",
            "Epoch :  154 \tLoss -  6.053720474243164\n",
            "Epoch :  155 \tLoss -  5.706212997436523\n",
            "Epoch :  156 \tLoss -  5.379317283630371\n",
            "Epoch :  157 \tLoss -  5.07149076461792\n",
            "Epoch :  158 \tLoss -  4.781911849975586\n",
            "Epoch :  159 \tLoss -  4.509284973144531\n",
            "Epoch :  160 \tLoss -  4.252692222595215\n",
            "Epoch :  161 \tLoss -  4.010977745056152\n",
            "Epoch :  162 \tLoss -  3.7833831310272217\n",
            "Epoch :  163 \tLoss -  3.56937837600708\n",
            "Epoch :  164 \tLoss -  3.367191791534424\n",
            "Epoch :  165 \tLoss -  3.177320957183838\n",
            "Epoch :  166 \tLoss -  2.9982101917266846\n",
            "Epoch :  167 \tLoss -  2.8294050693511963\n",
            "Epoch :  168 \tLoss -  2.6705727577209473\n",
            "Epoch :  169 \tLoss -  2.5207815170288086\n",
            "Epoch :  170 \tLoss -  2.379478931427002\n",
            "Epoch :  171 \tLoss -  2.246450901031494\n",
            "Epoch :  172 \tLoss -  2.120953321456909\n",
            "Epoch :  173 \tLoss -  2.0027856826782227\n",
            "Epoch :  174 \tLoss -  1.8911359310150146\n",
            "Epoch :  175 \tLoss -  1.785918951034546\n",
            "Epoch :  176 \tLoss -  1.686779260635376\n",
            "Epoch :  177 \tLoss -  1.5932239294052124\n",
            "Epoch :  178 \tLoss -  1.504973292350769\n",
            "Epoch :  179 \tLoss -  1.4216699600219727\n",
            "Epoch :  180 \tLoss -  1.3431192636489868\n",
            "Epoch :  181 \tLoss -  1.2691469192504883\n",
            "Epoch :  182 \tLoss -  1.1993476152420044\n",
            "Epoch :  183 \tLoss -  1.1333168745040894\n",
            "Epoch :  184 \tLoss -  1.071028709411621\n",
            "Epoch :  185 \tLoss -  1.0122781991958618\n",
            "Epoch :  186 \tLoss -  0.9568479061126709\n",
            "Epoch :  187 \tLoss -  0.9044472575187683\n",
            "Epoch :  188 \tLoss -  0.8550288081169128\n",
            "Epoch :  189 \tLoss -  0.8083377480506897\n",
            "Epoch :  190 \tLoss -  0.764212965965271\n",
            "Epoch :  191 \tLoss -  0.722636342048645\n",
            "Epoch :  192 \tLoss -  0.6833004355430603\n",
            "Epoch :  193 \tLoss -  0.6461920142173767\n",
            "Epoch :  194 \tLoss -  0.6111635565757751\n",
            "Epoch :  195 \tLoss -  0.5780541896820068\n",
            "Epoch :  196 \tLoss -  0.5467450618743896\n",
            "Epoch :  197 \tLoss -  0.5171672701835632\n",
            "Epoch :  198 \tLoss -  0.4892125725746155\n",
            "Epoch :  199 \tLoss -  0.46285122632980347\n",
            "Epoch :  200 \tLoss -  0.437911719083786\n",
            "Epoch :  201 \tLoss -  0.41431257128715515\n",
            "Epoch :  202 \tLoss -  0.392058402299881\n",
            "Epoch :  203 \tLoss -  0.3709680438041687\n",
            "Epoch :  204 \tLoss -  0.3511055111885071\n",
            "Epoch :  205 \tLoss -  0.3322884440422058\n",
            "Epoch :  206 \tLoss -  0.31449663639068604\n",
            "Epoch :  207 \tLoss -  0.2976722717285156\n",
            "Epoch :  208 \tLoss -  0.28177908062934875\n",
            "Epoch :  209 \tLoss -  0.26665616035461426\n",
            "Epoch :  210 \tLoss -  0.25245028734207153\n",
            "Epoch :  211 \tLoss -  0.23904001712799072\n",
            "Epoch :  212 \tLoss -  0.2262805700302124\n",
            "Epoch :  213 \tLoss -  0.21426211297512054\n",
            "Epoch :  214 \tLoss -  0.20284037292003632\n",
            "Epoch :  215 \tLoss -  0.19209729135036469\n",
            "Epoch :  216 \tLoss -  0.18192967772483826\n",
            "Epoch :  217 \tLoss -  0.17230379581451416\n",
            "Epoch :  218 \tLoss -  0.1631740927696228\n",
            "Epoch :  219 \tLoss -  0.15456357598304749\n",
            "Epoch :  220 \tLoss -  0.14641022682189941\n",
            "Epoch :  221 \tLoss -  0.13868026435375214\n",
            "Epoch :  222 \tLoss -  0.13137225806713104\n",
            "Epoch :  223 \tLoss -  0.1244700700044632\n",
            "Epoch :  224 \tLoss -  0.11789985746145248\n",
            "Epoch :  225 \tLoss -  0.11169429868459702\n",
            "Epoch :  226 \tLoss -  0.10582942515611649\n",
            "Epoch :  227 \tLoss -  0.10028209537267685\n",
            "Epoch :  228 \tLoss -  0.0950116440653801\n",
            "Epoch :  229 \tLoss -  0.0900319516658783\n",
            "Epoch :  230 \tLoss -  0.08532208949327469\n",
            "Epoch :  231 \tLoss -  0.0808744877576828\n",
            "Epoch :  232 \tLoss -  0.07663606107234955\n",
            "Epoch :  233 \tLoss -  0.07263818383216858\n",
            "Epoch :  234 \tLoss -  0.06884931027889252\n",
            "Epoch :  235 \tLoss -  0.06526386737823486\n",
            "Epoch :  236 \tLoss -  0.06185757368803024\n",
            "Epoch :  237 \tLoss -  0.058633312582969666\n",
            "Epoch :  238 \tLoss -  0.05560965836048126\n",
            "Epoch :  239 \tLoss -  0.0527082234621048\n",
            "Epoch :  240 \tLoss -  0.04998461902141571\n",
            "Epoch :  241 \tLoss -  0.047389306128025055\n",
            "Epoch :  242 \tLoss -  0.04491938278079033\n",
            "Epoch :  243 \tLoss -  0.04259704425930977\n",
            "Epoch :  244 \tLoss -  0.040390029549598694\n",
            "Epoch :  245 \tLoss -  0.038306672126054764\n",
            "Epoch :  246 \tLoss -  0.036330197006464005\n",
            "Epoch :  247 \tLoss -  0.03447295352816582\n",
            "Epoch :  248 \tLoss -  0.032686054706573486\n",
            "Epoch :  249 \tLoss -  0.03100796788930893\n",
            "Epoch :  250 \tLoss -  0.029417388141155243\n",
            "Epoch :  251 \tLoss -  0.027902305126190186\n",
            "Epoch :  252 \tLoss -  0.02647228166460991\n",
            "Epoch :  253 \tLoss -  0.025115059688687325\n",
            "Epoch :  254 \tLoss -  0.023828208446502686\n",
            "Epoch :  255 \tLoss -  0.02261095494031906\n",
            "Epoch :  256 \tLoss -  0.021465767174959183\n",
            "Epoch :  257 \tLoss -  0.020363830029964447\n",
            "Epoch :  258 \tLoss -  0.019336601719260216\n",
            "Epoch :  259 \tLoss -  0.018351469188928604\n",
            "Epoch :  260 \tLoss -  0.017419418320059776\n",
            "Epoch :  261 \tLoss -  0.016537262126803398\n",
            "Epoch :  262 \tLoss -  0.01570543274283409\n",
            "Epoch :  263 \tLoss -  0.014907215721905231\n",
            "Epoch :  264 \tLoss -  0.01415596529841423\n",
            "Epoch :  265 \tLoss -  0.013443086296319962\n",
            "Epoch :  266 \tLoss -  0.012768350541591644\n",
            "Epoch :  267 \tLoss -  0.012129981070756912\n",
            "Epoch :  268 \tLoss -  0.011522620916366577\n",
            "Epoch :  269 \tLoss -  0.010946789756417274\n",
            "Epoch :  270 \tLoss -  0.010397890582680702\n",
            "Epoch :  271 \tLoss -  0.009876780211925507\n",
            "Epoch :  272 \tLoss -  0.009390590712428093\n",
            "Epoch :  273 \tLoss -  0.008923342451453209\n",
            "Epoch :  274 \tLoss -  0.008482350967824459\n",
            "Epoch :  275 \tLoss -  0.008064772933721542\n",
            "Epoch :  276 \tLoss -  0.007665998302400112\n",
            "Epoch :  277 \tLoss -  0.007289881817996502\n",
            "Epoch :  278 \tLoss -  0.006930171512067318\n",
            "Epoch :  279 \tLoss -  0.006591317243874073\n",
            "Epoch :  280 \tLoss -  0.006267163436859846\n",
            "Epoch :  281 \tLoss -  0.005965723656117916\n",
            "Epoch :  282 \tLoss -  0.00567483389750123\n",
            "Epoch :  283 \tLoss -  0.005402594339102507\n",
            "Epoch :  284 \tLoss -  0.00514476653188467\n",
            "Epoch :  285 \tLoss -  0.004902206361293793\n",
            "Epoch :  286 \tLoss -  0.004665222018957138\n",
            "Epoch :  287 \tLoss -  0.004447077866643667\n",
            "Epoch :  288 \tLoss -  0.0042327819392085075\n",
            "Epoch :  289 \tLoss -  0.004035674035549164\n",
            "Epoch :  290 \tLoss -  0.0038455782923847437\n",
            "Epoch :  291 \tLoss -  0.0036641345359385014\n",
            "Epoch :  292 \tLoss -  0.0034933763090521097\n",
            "Epoch :  293 \tLoss -  0.003333396278321743\n",
            "Epoch :  294 \tLoss -  0.0031789480708539486\n",
            "Epoch :  295 \tLoss -  0.0030337893404066563\n",
            "Epoch :  296 \tLoss -  0.002893844386562705\n",
            "Epoch :  297 \tLoss -  0.0027616401202976704\n",
            "Epoch :  298 \tLoss -  0.00263718469068408\n",
            "Epoch :  299 \tLoss -  0.002518144901841879\n",
            "Epoch :  300 \tLoss -  0.002404548693448305\n",
            "Epoch :  301 \tLoss -  0.0022958219051361084\n",
            "Epoch :  302 \tLoss -  0.002195906825363636\n",
            "Epoch :  303 \tLoss -  0.0020967000164091587\n",
            "Epoch :  304 \tLoss -  0.0020060394890606403\n",
            "Epoch :  305 \tLoss -  0.0019176501082256436\n",
            "Epoch :  306 \tLoss -  0.0018326950957998633\n",
            "Epoch :  307 \tLoss -  0.0017542884452268481\n",
            "Epoch :  308 \tLoss -  0.001677536522038281\n",
            "Epoch :  309 \tLoss -  0.0016074939630925655\n",
            "Epoch :  310 \tLoss -  0.001537482487037778\n",
            "Epoch :  311 \tLoss -  0.0014719496248289943\n",
            "Epoch :  312 \tLoss -  0.001408873707987368\n",
            "Epoch :  313 \tLoss -  0.0013501830399036407\n",
            "Epoch :  314 \tLoss -  0.0012965600471943617\n",
            "Epoch :  315 \tLoss -  0.0012418271508067846\n",
            "Epoch :  316 \tLoss -  0.0011911869514733553\n",
            "Epoch :  317 \tLoss -  0.0011422674870118499\n",
            "Epoch :  318 \tLoss -  0.0010957380291074514\n",
            "Epoch :  319 \tLoss -  0.0010521594667807221\n",
            "Epoch :  320 \tLoss -  0.0010093632154166698\n",
            "Epoch :  321 \tLoss -  0.0009696758352220058\n",
            "Epoch :  322 \tLoss -  0.0009321614052169025\n",
            "Epoch :  323 \tLoss -  0.00089500832837075\n",
            "Epoch :  324 \tLoss -  0.000859977095387876\n",
            "Epoch :  325 \tLoss -  0.000826713046990335\n",
            "Epoch :  326 \tLoss -  0.0007954726461321115\n",
            "Epoch :  327 \tLoss -  0.0007639848627150059\n",
            "Epoch :  328 \tLoss -  0.0007350962841883302\n",
            "Epoch :  329 \tLoss -  0.0007075476460158825\n",
            "Epoch :  330 \tLoss -  0.0006808207836002111\n",
            "Epoch :  331 \tLoss -  0.0006557504530064762\n",
            "Epoch :  332 \tLoss -  0.0006309359450824559\n",
            "Epoch :  333 \tLoss -  0.0006083480548113585\n",
            "Epoch :  334 \tLoss -  0.0005871349130757153\n",
            "Epoch :  335 \tLoss -  0.0005647114012390375\n",
            "Epoch :  336 \tLoss -  0.0005444183479994535\n",
            "Epoch :  337 \tLoss -  0.0005249419482424855\n",
            "Epoch :  338 \tLoss -  0.0005068745813332498\n",
            "Epoch :  339 \tLoss -  0.0004887504619546235\n",
            "Epoch :  340 \tLoss -  0.0004717405536212027\n",
            "Epoch :  341 \tLoss -  0.00045588077045977116\n",
            "Epoch :  342 \tLoss -  0.0004394954885356128\n",
            "Epoch :  343 \tLoss -  0.00042445119470357895\n",
            "Epoch :  344 \tLoss -  0.00040963158244267106\n",
            "Epoch :  345 \tLoss -  0.0003961635520681739\n",
            "Epoch :  346 \tLoss -  0.0003839911369141191\n",
            "Epoch :  347 \tLoss -  0.00037101746420376003\n",
            "Epoch :  348 \tLoss -  0.0003579825861379504\n",
            "Epoch :  349 \tLoss -  0.00034669716842472553\n",
            "Epoch :  350 \tLoss -  0.00033531273948028684\n",
            "Epoch :  351 \tLoss -  0.0003250892914365977\n",
            "Epoch :  352 \tLoss -  0.000315080804284662\n",
            "Epoch :  353 \tLoss -  0.0003045965568162501\n",
            "Epoch :  354 \tLoss -  0.0002957816468551755\n",
            "Epoch :  355 \tLoss -  0.00028643885161727667\n",
            "Epoch :  356 \tLoss -  0.00027800159296020865\n",
            "Epoch :  357 \tLoss -  0.00026978066307492554\n",
            "Epoch :  358 \tLoss -  0.0002610019873827696\n",
            "Epoch :  359 \tLoss -  0.0002530371129978448\n",
            "Epoch :  360 \tLoss -  0.00024579960154369473\n",
            "Epoch :  361 \tLoss -  0.00023822906950954348\n",
            "Epoch :  362 \tLoss -  0.00023143176804296672\n",
            "Epoch :  363 \tLoss -  0.00022468196402769536\n",
            "Epoch :  364 \tLoss -  0.00021784927230328321\n",
            "Epoch :  365 \tLoss -  0.00021200776973273605\n",
            "Epoch :  366 \tLoss -  0.00020681688329204917\n",
            "Epoch :  367 \tLoss -  0.00020093866623938084\n",
            "Epoch :  368 \tLoss -  0.00019533460726961493\n",
            "Epoch :  369 \tLoss -  0.00019035387958865613\n",
            "Epoch :  370 \tLoss -  0.00018480137805454433\n",
            "Epoch :  371 \tLoss -  0.0001798779412638396\n",
            "Epoch :  372 \tLoss -  0.00017536684754304588\n",
            "Epoch :  373 \tLoss -  0.00017045787535607815\n",
            "Epoch :  374 \tLoss -  0.00016632955521345139\n",
            "Epoch :  375 \tLoss -  0.00016217734082601964\n",
            "Epoch :  376 \tLoss -  0.00015794100181665272\n",
            "Epoch :  377 \tLoss -  0.00015361947589553893\n",
            "Epoch :  378 \tLoss -  0.0001498785859439522\n",
            "Epoch :  379 \tLoss -  0.00014640958397649229\n",
            "Epoch :  380 \tLoss -  0.00014255510177463293\n",
            "Epoch :  381 \tLoss -  0.0001391768455505371\n",
            "Epoch :  382 \tLoss -  0.00013564169057644904\n",
            "Epoch :  383 \tLoss -  0.00013241438136901706\n",
            "Epoch :  384 \tLoss -  0.00012945127673447132\n",
            "Epoch :  385 \tLoss -  0.00012623325164895505\n",
            "Epoch :  386 \tLoss -  0.00012336917279753834\n",
            "Epoch :  387 \tLoss -  0.00012046442134305835\n",
            "Epoch :  388 \tLoss -  0.00011775796883739531\n",
            "Epoch :  389 \tLoss -  0.00011468786397017539\n",
            "Epoch :  390 \tLoss -  0.00011183804599568248\n",
            "Epoch :  391 \tLoss -  0.00010906787792919204\n",
            "Epoch :  392 \tLoss -  0.00010692906653275713\n",
            "Epoch :  393 \tLoss -  0.00010453806316945702\n",
            "Epoch :  394 \tLoss -  0.00010246292367810383\n",
            "Epoch :  395 \tLoss -  0.00010001398914027959\n",
            "Epoch :  396 \tLoss -  9.806025627767667e-05\n",
            "Epoch :  397 \tLoss -  9.591618436388671e-05\n",
            "Epoch :  398 \tLoss -  9.398389374837279e-05\n",
            "Epoch :  399 \tLoss -  9.16689241421409e-05\n",
            "Epoch :  400 \tLoss -  8.968726615421474e-05\n",
            "Epoch :  401 \tLoss -  8.809631981421262e-05\n",
            "Epoch :  402 \tLoss -  8.61790613271296e-05\n",
            "Epoch :  403 \tLoss -  8.433635230176151e-05\n",
            "Epoch :  404 \tLoss -  8.292548591271043e-05\n",
            "Epoch :  405 \tLoss -  8.137834083754569e-05\n",
            "Epoch :  406 \tLoss -  7.960464427014813e-05\n",
            "Epoch :  407 \tLoss -  7.823537453077734e-05\n",
            "Epoch :  408 \tLoss -  7.670436025364324e-05\n",
            "Epoch :  409 \tLoss -  7.498521881643683e-05\n",
            "Epoch :  410 \tLoss -  7.354299305006862e-05\n",
            "Epoch :  411 \tLoss -  7.198516686912626e-05\n",
            "Epoch :  412 \tLoss -  7.052114233374596e-05\n",
            "Epoch :  413 \tLoss -  6.892847159178928e-05\n",
            "Epoch :  414 \tLoss -  6.764731369912624e-05\n",
            "Epoch :  415 \tLoss -  6.638361810473725e-05\n",
            "Epoch :  416 \tLoss -  6.499813753180206e-05\n",
            "Epoch :  417 \tLoss -  6.40619546175003e-05\n",
            "Epoch :  418 \tLoss -  6.31029179203324e-05\n",
            "Epoch :  419 \tLoss -  6.181221397127956e-05\n",
            "Epoch :  420 \tLoss -  6.058690996724181e-05\n",
            "Epoch :  421 \tLoss -  5.9499318012967706e-05\n",
            "Epoch :  422 \tLoss -  5.835758202010766e-05\n",
            "Epoch :  423 \tLoss -  5.7661196478875354e-05\n",
            "Epoch :  424 \tLoss -  5.652949766954407e-05\n",
            "Epoch :  425 \tLoss -  5.536495154956356e-05\n",
            "Epoch :  426 \tLoss -  5.452948244055733e-05\n",
            "Epoch :  427 \tLoss -  5.345959652913734e-05\n",
            "Epoch :  428 \tLoss -  5.2740575483767316e-05\n",
            "Epoch :  429 \tLoss -  5.1793846068903804e-05\n",
            "Epoch :  430 \tLoss -  5.09007295477204e-05\n",
            "Epoch :  431 \tLoss -  5.000485180062242e-05\n",
            "Epoch :  432 \tLoss -  4.9285186833003536e-05\n",
            "Epoch :  433 \tLoss -  4.8447545850649476e-05\n",
            "Epoch :  434 \tLoss -  4.760987212648615e-05\n",
            "Epoch :  435 \tLoss -  4.6789777115918696e-05\n",
            "Epoch :  436 \tLoss -  4.612718475982547e-05\n",
            "Epoch :  437 \tLoss -  4.537979111773893e-05\n",
            "Epoch :  438 \tLoss -  4.4568467274075374e-05\n",
            "Epoch :  439 \tLoss -  4.38433162344154e-05\n",
            "Epoch :  440 \tLoss -  4.303218884160742e-05\n",
            "Epoch :  441 \tLoss -  4.264802555553615e-05\n",
            "Epoch :  442 \tLoss -  4.196191002847627e-05\n",
            "Epoch :  443 \tLoss -  4.1345396311953664e-05\n",
            "Epoch :  444 \tLoss -  4.0857717976905406e-05\n",
            "Epoch :  445 \tLoss -  4.017951141577214e-05\n",
            "Epoch :  446 \tLoss -  3.9429916796507314e-05\n",
            "Epoch :  447 \tLoss -  3.8709371438017115e-05\n",
            "Epoch :  448 \tLoss -  3.8145353755680844e-05\n",
            "Epoch :  449 \tLoss -  3.769942850340158e-05\n",
            "Epoch :  450 \tLoss -  3.71531568816863e-05\n",
            "Epoch :  451 \tLoss -  3.6590972740668803e-05\n",
            "Epoch :  452 \tLoss -  3.617348556872457e-05\n",
            "Epoch :  453 \tLoss -  3.572249261196703e-05\n",
            "Epoch :  454 \tLoss -  3.515368007356301e-05\n",
            "Epoch :  455 \tLoss -  3.475456469459459e-05\n",
            "Epoch :  456 \tLoss -  3.430828655837104e-05\n",
            "Epoch :  457 \tLoss -  3.3658907341305166e-05\n",
            "Epoch :  458 \tLoss -  3.319888492114842e-05\n",
            "Epoch :  459 \tLoss -  3.2783289498183876e-05\n",
            "Epoch :  460 \tLoss -  3.2367741368943825e-05\n",
            "Epoch :  461 \tLoss -  3.191509676980786e-05\n",
            "Epoch :  462 \tLoss -  3.1420997402165085e-05\n",
            "Epoch :  463 \tLoss -  3.111168189207092e-05\n",
            "Epoch :  464 \tLoss -  3.078613372053951e-05\n",
            "Epoch :  465 \tLoss -  3.0262661312008277e-05\n",
            "Epoch :  466 \tLoss -  2.9815237212460488e-05\n",
            "Epoch :  467 \tLoss -  2.942864375654608e-05\n",
            "Epoch :  468 \tLoss -  2.9133076168363914e-05\n",
            "Epoch :  469 \tLoss -  2.8731414204230532e-05\n",
            "Epoch :  470 \tLoss -  2.8345360988168977e-05\n",
            "Epoch :  471 \tLoss -  2.78850147878984e-05\n",
            "Epoch :  472 \tLoss -  2.7597085136221722e-05\n",
            "Epoch :  473 \tLoss -  2.7279453206574544e-05\n",
            "Epoch :  474 \tLoss -  2.69491156359436e-05\n",
            "Epoch :  475 \tLoss -  2.685860454221256e-05\n",
            "Epoch :  476 \tLoss -  2.6443040042067878e-05\n",
            "Epoch :  477 \tLoss -  2.6154280931223184e-05\n",
            "Epoch :  478 \tLoss -  2.587346898508258e-05\n",
            "Epoch :  479 \tLoss -  2.55917948379647e-05\n",
            "Epoch :  480 \tLoss -  2.5232879124814644e-05\n",
            "Epoch :  481 \tLoss -  2.4878341719158925e-05\n",
            "Epoch :  482 \tLoss -  2.4616445443825796e-05\n",
            "Epoch :  483 \tLoss -  2.4335515263373964e-05\n",
            "Epoch :  484 \tLoss -  2.407712599961087e-05\n",
            "Epoch :  485 \tLoss -  2.369704634475056e-05\n",
            "Epoch :  486 \tLoss -  2.353527452214621e-05\n",
            "Epoch :  487 \tLoss -  2.3303868147195317e-05\n",
            "Epoch :  488 \tLoss -  2.3109172616386786e-05\n",
            "Epoch :  489 \tLoss -  2.2844822524348274e-05\n",
            "Epoch :  490 \tLoss -  2.260793189634569e-05\n",
            "Epoch :  491 \tLoss -  2.2361038645613007e-05\n",
            "Epoch :  492 \tLoss -  2.2150623408379033e-05\n",
            "Epoch :  493 \tLoss -  2.1840085537405685e-05\n",
            "Epoch :  494 \tLoss -  2.1535488485824317e-05\n",
            "Epoch :  495 \tLoss -  2.1295158148859628e-05\n",
            "Epoch :  496 \tLoss -  2.1131683752173558e-05\n",
            "Epoch :  497 \tLoss -  2.0885210687993094e-05\n",
            "Epoch :  498 \tLoss -  2.07063330890378e-05\n",
            "Epoch :  499 \tLoss -  2.0537752789095975e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2x8scc0nStf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f58eb35-e2f1-4f8c-eac9-0872880451af"
      },
      "source": [
        "######################################################\n",
        "################# PyTorch nn module ##################\n",
        "######################################################\n",
        "\n",
        "\"\"\"\n",
        "nn package provides a set of Modules which can be used to build up a\n",
        "neural network.\n",
        "\"\"\"\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "\n",
        "batch_size, input_size, hidden_size, output_size = 64, 1024, 512, 10\n",
        "\n",
        "# random valued tensors for input and output\n",
        "x = torch.randn(batch_size, input_size, device=device)\n",
        "y = torch.randn(batch_size, output_size, device=device)\n",
        "\n",
        "# nn.Sequential can be used to define our model as a sequence of layers\n",
        "# in this case, the input goes through the following transformations\n",
        "# input --> Linear --> activation(Relu) --> Linear\n",
        "# Linear module computes output using a linear function\n",
        "# relu module applies relu function to the ouptut obtained from the last layer\n",
        "model = torch.nn.Sequential(\n",
        "          torch.nn.Linear(input_size, hidden_size),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(hidden_size, output_size),\n",
        "        ).to(device)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# This is mean squared loss and elementwise_mean means the loss is mean\n",
        "# of squared errors.\n",
        "loss_fn = torch.nn.MSELoss(reduction='elementwise_mean')\n",
        "\n",
        "# optim package  provides implementations of commonly used optimization \n",
        "# algorithms. The optimizer updates the weights of the model for us.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 500\n",
        "for e in range(epochs):\n",
        "\n",
        "    # calculate output\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # computing and printing loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    print(\"Epoch : \", e,\"\\tLoss - \", loss.item())\n",
        "    \n",
        "    # zero out all the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update the parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  0 \tLoss -  1.1709027290344238\n",
            "Epoch :  1 \tLoss -  1.1026450395584106\n",
            "Epoch :  2 \tLoss -  1.0374784469604492\n",
            "Epoch :  3 \tLoss -  0.9752761721611023\n",
            "Epoch :  4 \tLoss -  0.916059136390686\n",
            "Epoch :  5 \tLoss -  0.859638512134552\n",
            "Epoch :  6 \tLoss -  0.8060597777366638\n",
            "Epoch :  7 \tLoss -  0.7551005482673645\n",
            "Epoch :  8 \tLoss -  0.7068974375724792\n",
            "Epoch :  9 \tLoss -  0.6613147854804993\n",
            "Epoch :  10 \tLoss -  0.6182137727737427\n",
            "Epoch :  11 \tLoss -  0.5773984789848328\n",
            "Epoch :  12 \tLoss -  0.5387842655181885\n",
            "Epoch :  13 \tLoss -  0.5021977424621582\n",
            "Epoch :  14 \tLoss -  0.4675353169441223\n",
            "Epoch :  15 \tLoss -  0.43473315238952637\n",
            "Epoch :  16 \tLoss -  0.4037090837955475\n",
            "Epoch :  17 \tLoss -  0.37448650598526\n",
            "Epoch :  18 \tLoss -  0.3468888998031616\n",
            "Epoch :  19 \tLoss -  0.32081228494644165\n",
            "Epoch :  20 \tLoss -  0.29626375436782837\n",
            "Epoch :  21 \tLoss -  0.2731039524078369\n",
            "Epoch :  22 \tLoss -  0.25130006670951843\n",
            "Epoch :  23 \tLoss -  0.23075710237026215\n",
            "Epoch :  24 \tLoss -  0.21147333085536957\n",
            "Epoch :  25 \tLoss -  0.1933356672525406\n",
            "Epoch :  26 \tLoss -  0.17634284496307373\n",
            "Epoch :  27 \tLoss -  0.16047684848308563\n",
            "Epoch :  28 \tLoss -  0.14567862451076508\n",
            "Epoch :  29 \tLoss -  0.131924107670784\n",
            "Epoch :  30 \tLoss -  0.11914395540952682\n",
            "Epoch :  31 \tLoss -  0.1072714552283287\n",
            "Epoch :  32 \tLoss -  0.09627299755811691\n",
            "Epoch :  33 \tLoss -  0.08611154556274414\n",
            "Epoch :  34 \tLoss -  0.07675599306821823\n",
            "Epoch :  35 \tLoss -  0.06814916431903839\n",
            "Epoch :  36 \tLoss -  0.06026853248476982\n",
            "Epoch :  37 \tLoss -  0.05306527763605118\n",
            "Epoch :  38 \tLoss -  0.046506788581609726\n",
            "Epoch :  39 \tLoss -  0.04056065157055855\n",
            "Epoch :  40 \tLoss -  0.03518608212471008\n",
            "Epoch :  41 \tLoss -  0.030346697196364403\n",
            "Epoch :  42 \tLoss -  0.02601810172200203\n",
            "Epoch :  43 \tLoss -  0.022169871255755424\n",
            "Epoch :  44 \tLoss -  0.018763137981295586\n",
            "Epoch :  45 \tLoss -  0.01576067879796028\n",
            "Epoch :  46 \tLoss -  0.013136162422597408\n",
            "Epoch :  47 \tLoss -  0.0108549315482378\n",
            "Epoch :  48 \tLoss -  0.008885325863957405\n",
            "Epoch :  49 \tLoss -  0.0071988338604569435\n",
            "Epoch :  50 \tLoss -  0.005769295152276754\n",
            "Epoch :  51 \tLoss -  0.004570114891976118\n",
            "Epoch :  52 \tLoss -  0.0035766817163676023\n",
            "Epoch :  53 \tLoss -  0.002763994736596942\n",
            "Epoch :  54 \tLoss -  0.002112745074555278\n",
            "Epoch :  55 \tLoss -  0.0015997805166989565\n",
            "Epoch :  56 \tLoss -  0.001205518958158791\n",
            "Epoch :  57 \tLoss -  0.0009111436083912849\n",
            "Epoch :  58 \tLoss -  0.0006996539887040854\n",
            "Epoch :  59 \tLoss -  0.0005564730963669717\n",
            "Epoch :  60 \tLoss -  0.00046751974150538445\n",
            "Epoch :  61 \tLoss -  0.0004209750040899962\n",
            "Epoch :  62 \tLoss -  0.0004064452077727765\n",
            "Epoch :  63 \tLoss -  0.0004150372988078743\n",
            "Epoch :  64 \tLoss -  0.00043922793702222407\n",
            "Epoch :  65 \tLoss -  0.0004726206825580448\n",
            "Epoch :  66 \tLoss -  0.00051016645738855\n",
            "Epoch :  67 \tLoss -  0.0005478549282997847\n",
            "Epoch :  68 \tLoss -  0.0005823803949169815\n",
            "Epoch :  69 \tLoss -  0.0006114934221841395\n",
            "Epoch :  70 \tLoss -  0.0006337255472317338\n",
            "Epoch :  71 \tLoss -  0.000648148765321821\n",
            "Epoch :  72 \tLoss -  0.0006543718627654016\n",
            "Epoch :  73 \tLoss -  0.0006524535128846765\n",
            "Epoch :  74 \tLoss -  0.0006427644984796643\n",
            "Epoch :  75 \tLoss -  0.0006259653018787503\n",
            "Epoch :  76 \tLoss -  0.0006028892821632326\n",
            "Epoch :  77 \tLoss -  0.0005744922091253102\n",
            "Epoch :  78 \tLoss -  0.000541801389772445\n",
            "Epoch :  79 \tLoss -  0.0005058505339547992\n",
            "Epoch :  80 \tLoss -  0.00046765952720306814\n",
            "Epoch :  81 \tLoss -  0.0004281763103790581\n",
            "Epoch :  82 \tLoss -  0.00038828604738228023\n",
            "Epoch :  83 \tLoss -  0.00034877960570156574\n",
            "Epoch :  84 \tLoss -  0.0003103300114162266\n",
            "Epoch :  85 \tLoss -  0.0002735115704126656\n",
            "Epoch :  86 \tLoss -  0.00023876666091382504\n",
            "Epoch :  87 \tLoss -  0.00020642916206270456\n",
            "Epoch :  88 \tLoss -  0.0001767349458532408\n",
            "Epoch :  89 \tLoss -  0.00014982458378653973\n",
            "Epoch :  90 \tLoss -  0.00012575910659506917\n",
            "Epoch :  91 \tLoss -  0.00010451338312122971\n",
            "Epoch :  92 \tLoss -  8.600165892858058e-05\n",
            "Epoch :  93 \tLoss -  7.009453111095354e-05\n",
            "Epoch :  94 \tLoss -  5.662453986587934e-05\n",
            "Epoch :  95 \tLoss -  4.539714063866995e-05\n",
            "Epoch :  96 \tLoss -  3.6199558962835e-05\n",
            "Epoch :  97 \tLoss -  2.880956708395388e-05\n",
            "Epoch :  98 \tLoss -  2.3003081878414378e-05\n",
            "Epoch :  99 \tLoss -  1.8559438103693537e-05\n",
            "Epoch :  100 \tLoss -  1.526661253592465e-05\n",
            "Epoch :  101 \tLoss -  1.2924790098622907e-05\n",
            "Epoch :  102 \tLoss -  1.1349760825396515e-05\n",
            "Epoch :  103 \tLoss -  1.0374508747190703e-05\n",
            "Epoch :  104 \tLoss -  9.851002687355503e-06\n",
            "Epoch :  105 \tLoss -  9.650418178352993e-06\n",
            "Epoch :  106 \tLoss -  9.663125638326164e-06\n",
            "Epoch :  107 \tLoss -  9.798622158996295e-06\n",
            "Epoch :  108 \tLoss -  9.983756172005087e-06\n",
            "Epoch :  109 \tLoss -  1.0162069884245284e-05\n",
            "Epoch :  110 \tLoss -  1.0291768376191612e-05\n",
            "Epoch :  111 \tLoss -  1.0344166184950154e-05\n",
            "Epoch :  112 \tLoss -  1.0301800102752168e-05\n",
            "Epoch :  113 \tLoss -  1.0156570169783663e-05\n",
            "Epoch :  114 \tLoss -  9.908227184496354e-06\n",
            "Epoch :  115 \tLoss -  9.56257190409815e-06\n",
            "Epoch :  116 \tLoss -  9.129642421612516e-06\n",
            "Epoch :  117 \tLoss -  8.622894711152185e-06\n",
            "Epoch :  118 \tLoss -  8.05756644695066e-06\n",
            "Epoch :  119 \tLoss -  7.449901204381604e-06\n",
            "Epoch :  120 \tLoss -  6.815974757046206e-06\n",
            "Epoch :  121 \tLoss -  6.171508175611962e-06\n",
            "Epoch :  122 \tLoss -  5.530558155442122e-06\n",
            "Epoch :  123 \tLoss -  4.905894456896931e-06\n",
            "Epoch :  124 \tLoss -  4.308049483370269e-06\n",
            "Epoch :  125 \tLoss -  3.7456782138178824e-06\n",
            "Epoch :  126 \tLoss -  3.225177806598367e-06\n",
            "Epoch :  127 \tLoss -  2.7509076971909963e-06\n",
            "Epoch :  128 \tLoss -  2.325360128452303e-06\n",
            "Epoch :  129 \tLoss -  1.949212673935108e-06\n",
            "Epoch :  130 \tLoss -  1.6217742313529016e-06\n",
            "Epoch :  131 \tLoss -  1.3411444115263293e-06\n",
            "Epoch :  132 \tLoss -  1.1044206758015207e-06\n",
            "Epoch :  133 \tLoss -  9.081397251975432e-07\n",
            "Epoch :  134 \tLoss -  7.481955321964051e-07\n",
            "Epoch :  135 \tLoss -  6.204227247508243e-07\n",
            "Epoch :  136 \tLoss -  5.204610147302446e-07\n",
            "Epoch :  137 \tLoss -  4.4407633481569064e-07\n",
            "Epoch :  138 \tLoss -  3.872249578762421e-07\n",
            "Epoch :  139 \tLoss -  3.460918094333465e-07\n",
            "Epoch :  140 \tLoss -  3.172497997638857e-07\n",
            "Epoch :  141 \tLoss -  2.9763120323877956e-07\n",
            "Epoch :  142 \tLoss -  2.8460127055041085e-07\n",
            "Epoch :  143 \tLoss -  2.7592642481977236e-07\n",
            "Epoch :  144 \tLoss -  2.697788374916854e-07\n",
            "Epoch :  145 \tLoss -  2.647302608238533e-07\n",
            "Epoch :  146 \tLoss -  2.597198829334957e-07\n",
            "Epoch :  147 \tLoss -  2.5396110459041665e-07\n",
            "Epoch :  148 \tLoss -  2.46987241325769e-07\n",
            "Epoch :  149 \tLoss -  2.385571349350357e-07\n",
            "Epoch :  150 \tLoss -  2.285946578695075e-07\n",
            "Epoch :  151 \tLoss -  2.1718638265610934e-07\n",
            "Epoch :  152 \tLoss -  2.0451186344416783e-07\n",
            "Epoch :  153 \tLoss -  1.9081080893101898e-07\n",
            "Epoch :  154 \tLoss -  1.763792596420899e-07\n",
            "Epoch :  155 \tLoss -  1.6152905857325095e-07\n",
            "Epoch :  156 \tLoss -  1.4655739732916118e-07\n",
            "Epoch :  157 \tLoss -  1.3173767854368634e-07\n",
            "Epoch :  158 \tLoss -  1.1733368410205003e-07\n",
            "Epoch :  159 \tLoss -  1.0356708202152731e-07\n",
            "Epoch :  160 \tLoss -  9.061005101784758e-08\n",
            "Epoch :  161 \tLoss -  7.859932082965315e-08\n",
            "Epoch :  162 \tLoss -  6.763477955473718e-08\n",
            "Epoch :  163 \tLoss -  5.776679046221034e-08\n",
            "Epoch :  164 \tLoss -  4.901539440993474e-08\n",
            "Epoch :  165 \tLoss -  4.137411124816026e-08\n",
            "Epoch :  166 \tLoss -  3.479778598602934e-08\n",
            "Epoch :  167 \tLoss -  2.9225821762679516e-08\n",
            "Epoch :  168 \tLoss -  2.4581055413364084e-08\n",
            "Epoch :  169 \tLoss -  2.076957272834079e-08\n",
            "Epoch :  170 \tLoss -  1.769498858550378e-08\n",
            "Epoch :  171 \tLoss -  1.5261434782587457e-08\n",
            "Epoch :  172 \tLoss -  1.3368228302113039e-08\n",
            "Epoch :  173 \tLoss -  1.1920437970047715e-08\n",
            "Epoch :  174 \tLoss -  1.083251444100597e-08\n",
            "Epoch :  175 \tLoss -  1.0026019126030405e-08\n",
            "Epoch :  176 \tLoss -  9.424859115370054e-09\n",
            "Epoch :  177 \tLoss -  8.972235399085093e-09\n",
            "Epoch :  178 \tLoss -  8.618049385233917e-09\n",
            "Epoch :  179 \tLoss -  8.315593547081335e-09\n",
            "Epoch :  180 \tLoss -  8.035425658192707e-09\n",
            "Epoch :  181 \tLoss -  7.749943797819014e-09\n",
            "Epoch :  182 \tLoss -  7.445041916298578e-09\n",
            "Epoch :  183 \tLoss -  7.10958003580231e-09\n",
            "Epoch :  184 \tLoss -  6.7388157276582206e-09\n",
            "Epoch :  185 \tLoss -  6.335591162809351e-09\n",
            "Epoch :  186 \tLoss -  5.904007061019456e-09\n",
            "Epoch :  187 \tLoss -  5.449187767680996e-09\n",
            "Epoch :  188 \tLoss -  4.982318113633255e-09\n",
            "Epoch :  189 \tLoss -  4.509812079334097e-09\n",
            "Epoch :  190 \tLoss -  4.045440427091762e-09\n",
            "Epoch :  191 \tLoss -  3.591910768818707e-09\n",
            "Epoch :  192 \tLoss -  3.1602283012688304e-09\n",
            "Epoch :  193 \tLoss -  2.7556292803154747e-09\n",
            "Epoch :  194 \tLoss -  2.382695374336663e-09\n",
            "Epoch :  195 \tLoss -  2.0440826808965085e-09\n",
            "Epoch :  196 \tLoss -  1.7425282328176195e-09\n",
            "Epoch :  197 \tLoss -  1.477662103788191e-09\n",
            "Epoch :  198 \tLoss -  1.2493298617854975e-09\n",
            "Epoch :  199 \tLoss -  1.0551499673994158e-09\n",
            "Epoch :  200 \tLoss -  8.936519302338297e-10\n",
            "Epoch :  201 \tLoss -  7.614671670985729e-10\n",
            "Epoch :  202 \tLoss -  6.553214082494208e-10\n",
            "Epoch :  203 \tLoss -  5.71247660285934e-10\n",
            "Epoch :  204 \tLoss -  5.063875430089126e-10\n",
            "Epoch :  205 \tLoss -  4.5697420847368164e-10\n",
            "Epoch :  206 \tLoss -  4.19372370163984e-10\n",
            "Epoch :  207 \tLoss -  3.9066369583729e-10\n",
            "Epoch :  208 \tLoss -  3.6815214765617554e-10\n",
            "Epoch :  209 \tLoss -  3.5041772261656945e-10\n",
            "Epoch :  210 \tLoss -  3.3502536855856135e-10\n",
            "Epoch :  211 \tLoss -  3.2068928068618163e-10\n",
            "Epoch :  212 \tLoss -  3.0670363471152484e-10\n",
            "Epoch :  213 \tLoss -  2.9227967268674604e-10\n",
            "Epoch :  214 \tLoss -  2.769147966485974e-10\n",
            "Epoch :  215 \tLoss -  2.6085542059739453e-10\n",
            "Epoch :  216 \tLoss -  2.4375257368092207e-10\n",
            "Epoch :  217 \tLoss -  2.2590196380178895e-10\n",
            "Epoch :  218 \tLoss -  2.075616623020693e-10\n",
            "Epoch :  219 \tLoss -  1.8898471676465078e-10\n",
            "Epoch :  220 \tLoss -  1.7067162960682225e-10\n",
            "Epoch :  221 \tLoss -  1.5252325791248467e-10\n",
            "Epoch :  222 \tLoss -  1.3515946695186187e-10\n",
            "Epoch :  223 \tLoss -  1.1876637173724447e-10\n",
            "Epoch :  224 \tLoss -  1.0369566316725809e-10\n",
            "Epoch :  225 \tLoss -  8.973210369189744e-11\n",
            "Epoch :  226 \tLoss -  7.740247331966543e-11\n",
            "Epoch :  227 \tLoss -  6.646716510516626e-11\n",
            "Epoch :  228 \tLoss -  5.6746652515871077e-11\n",
            "Epoch :  229 \tLoss -  4.846545387238166e-11\n",
            "Epoch :  230 \tLoss -  4.155761845758832e-11\n",
            "Epoch :  231 \tLoss -  3.565042011599573e-11\n",
            "Epoch :  232 \tLoss -  3.090101091673958e-11\n",
            "Epoch :  233 \tLoss -  2.696082072872752e-11\n",
            "Epoch :  234 \tLoss -  2.3804966678420136e-11\n",
            "Epoch :  235 \tLoss -  2.1407597916578425e-11\n",
            "Epoch :  236 \tLoss -  1.9412886229086546e-11\n",
            "Epoch :  237 \tLoss -  1.77839392517809e-11\n",
            "Epoch :  238 \tLoss -  1.650771706829257e-11\n",
            "Epoch :  239 \tLoss -  1.5430979410924195e-11\n",
            "Epoch :  240 \tLoss -  1.450818545162269e-11\n",
            "Epoch :  241 \tLoss -  1.3655707641058168e-11\n",
            "Epoch :  242 \tLoss -  1.2898951871898046e-11\n",
            "Epoch :  243 \tLoss -  1.2223325650262407e-11\n",
            "Epoch :  244 \tLoss -  1.1537067308442506e-11\n",
            "Epoch :  245 \tLoss -  1.0833843371027552e-11\n",
            "Epoch :  246 \tLoss -  1.019486405018366e-11\n",
            "Epoch :  247 \tLoss -  9.485479242343775e-12\n",
            "Epoch :  248 \tLoss -  8.736009311782755e-12\n",
            "Epoch :  249 \tLoss -  8.018137369336653e-12\n",
            "Epoch :  250 \tLoss -  7.32339883180444e-12\n",
            "Epoch :  251 \tLoss -  6.6040311240411764e-12\n",
            "Epoch :  252 \tLoss -  5.92598496315655e-12\n",
            "Epoch :  253 \tLoss -  5.3338561865901024e-12\n",
            "Epoch :  254 \tLoss -  4.717637305112765e-12\n",
            "Epoch :  255 \tLoss -  4.17824108644993e-12\n",
            "Epoch :  256 \tLoss -  3.670134508804157e-12\n",
            "Epoch :  257 \tLoss -  3.212049766790348e-12\n",
            "Epoch :  258 \tLoss -  2.771438677509619e-12\n",
            "Epoch :  259 \tLoss -  2.4072135344083145e-12\n",
            "Epoch :  260 \tLoss -  2.078964387794424e-12\n",
            "Epoch :  261 \tLoss -  1.7460398361521245e-12\n",
            "Epoch :  262 \tLoss -  1.4985585472179763e-12\n",
            "Epoch :  263 \tLoss -  1.2700869002346682e-12\n",
            "Epoch :  264 \tLoss -  1.0668765133489688e-12\n",
            "Epoch :  265 \tLoss -  9.021809449678841e-13\n",
            "Epoch :  266 \tLoss -  7.681621181157561e-13\n",
            "Epoch :  267 \tLoss -  6.570974233482962e-13\n",
            "Epoch :  268 \tLoss -  5.686275018554343e-13\n",
            "Epoch :  269 \tLoss -  5.046012436019098e-13\n",
            "Epoch :  270 \tLoss -  4.43755790576969e-13\n",
            "Epoch :  271 \tLoss -  4.057681382736711e-13\n",
            "Epoch :  272 \tLoss -  3.7410271278362495e-13\n",
            "Epoch :  273 \tLoss -  3.459450878973841e-13\n",
            "Epoch :  274 \tLoss -  3.2840562409243435e-13\n",
            "Epoch :  275 \tLoss -  3.058974243613094e-13\n",
            "Epoch :  276 \tLoss -  2.9481687815850754e-13\n",
            "Epoch :  277 \tLoss -  2.7484630782219355e-13\n",
            "Epoch :  278 \tLoss -  2.605823814106484e-13\n",
            "Epoch :  279 \tLoss -  2.4295317277087114e-13\n",
            "Epoch :  280 \tLoss -  2.3243058411108464e-13\n",
            "Epoch :  281 \tLoss -  2.1700515191215575e-13\n",
            "Epoch :  282 \tLoss -  2.022395109452929e-13\n",
            "Epoch :  283 \tLoss -  1.8994272029792397e-13\n",
            "Epoch :  284 \tLoss -  1.7381697481073238e-13\n",
            "Epoch :  285 \tLoss -  1.6559777366639133e-13\n",
            "Epoch :  286 \tLoss -  1.5384404633469323e-13\n",
            "Epoch :  287 \tLoss -  1.395318593739453e-13\n",
            "Epoch :  288 \tLoss -  1.3220877843021783e-13\n",
            "Epoch :  289 \tLoss -  1.2257112913614115e-13\n",
            "Epoch :  290 \tLoss -  1.1035793725253443e-13\n",
            "Epoch :  291 \tLoss -  1.0377820564705376e-13\n",
            "Epoch :  292 \tLoss -  9.308607487254483e-14\n",
            "Epoch :  293 \tLoss -  8.513675197772616e-14\n",
            "Epoch :  294 \tLoss -  7.585217312111939e-14\n",
            "Epoch :  295 \tLoss -  6.872447896140096e-14\n",
            "Epoch :  296 \tLoss -  6.104240512583639e-14\n",
            "Epoch :  297 \tLoss -  5.714759178029882e-14\n",
            "Epoch :  298 \tLoss -  5.19067109444344e-14\n",
            "Epoch :  299 \tLoss -  4.6952594806928816e-14\n",
            "Epoch :  300 \tLoss -  4.337294886853742e-14\n",
            "Epoch :  301 \tLoss -  4.1356627595180023e-14\n",
            "Epoch :  302 \tLoss -  3.4467623931866073e-14\n",
            "Epoch :  303 \tLoss -  3.1295980211639915e-14\n",
            "Epoch :  304 \tLoss -  2.983555988530194e-14\n",
            "Epoch :  305 \tLoss -  2.5568526381422943e-14\n",
            "Epoch :  306 \tLoss -  2.4055793300262543e-14\n",
            "Epoch :  307 \tLoss -  2.341652567650846e-14\n",
            "Epoch :  308 \tLoss -  2.153733564917975e-14\n",
            "Epoch :  309 \tLoss -  2.031035421497327e-14\n",
            "Epoch :  310 \tLoss -  1.8857680851970884e-14\n",
            "Epoch :  311 \tLoss -  1.7811751016174118e-14\n",
            "Epoch :  312 \tLoss -  1.5834713436844484e-14\n",
            "Epoch :  313 \tLoss -  1.43866665678e-14\n",
            "Epoch :  314 \tLoss -  1.4336861030501448e-14\n",
            "Epoch :  315 \tLoss -  1.378463366537427e-14\n",
            "Epoch :  316 \tLoss -  1.2484793845176766e-14\n",
            "Epoch :  317 \tLoss -  1.239034120122844e-14\n",
            "Epoch :  318 \tLoss -  1.2113742592302548e-14\n",
            "Epoch :  319 \tLoss -  1.1601676447336486e-14\n",
            "Epoch :  320 \tLoss -  1.052195084397722e-14\n",
            "Epoch :  321 \tLoss -  1.0030607360065735e-14\n",
            "Epoch :  322 \tLoss -  9.958609559549119e-15\n",
            "Epoch :  323 \tLoss -  9.672853677430461e-15\n",
            "Epoch :  324 \tLoss -  1.0590053139969413e-14\n",
            "Epoch :  325 \tLoss -  1.0151245180545481e-14\n",
            "Epoch :  326 \tLoss -  9.610014844172506e-15\n",
            "Epoch :  327 \tLoss -  8.660780595568397e-15\n",
            "Epoch :  328 \tLoss -  8.760881255209015e-15\n",
            "Epoch :  329 \tLoss -  8.025289044693174e-15\n",
            "Epoch :  330 \tLoss -  7.653321302170026e-15\n",
            "Epoch :  331 \tLoss -  7.751875279648959e-15\n",
            "Epoch :  332 \tLoss -  7.53678312115499e-15\n",
            "Epoch :  333 \tLoss -  7.460861864026693e-15\n",
            "Epoch :  334 \tLoss -  7.288964150677958e-15\n",
            "Epoch :  335 \tLoss -  7.110645080556088e-15\n",
            "Epoch :  336 \tLoss -  7.216580103137395e-15\n",
            "Epoch :  337 \tLoss -  6.6812874677246725e-15\n",
            "Epoch :  338 \tLoss -  6.684537533143287e-15\n",
            "Epoch :  339 \tLoss -  6.625413362875516e-15\n",
            "Epoch :  340 \tLoss -  6.428304560884704e-15\n",
            "Epoch :  341 \tLoss -  6.2244961517975844e-15\n",
            "Epoch :  342 \tLoss -  6.384730644979101e-15\n",
            "Epoch :  343 \tLoss -  6.683895058652795e-15\n",
            "Epoch :  344 \tLoss -  6.3537182276812796e-15\n",
            "Epoch :  345 \tLoss -  5.84556622196448e-15\n",
            "Epoch :  346 \tLoss -  5.709477249977394e-15\n",
            "Epoch :  347 \tLoss -  5.868226047369427e-15\n",
            "Epoch :  348 \tLoss -  5.94715342442753e-15\n",
            "Epoch :  349 \tLoss -  5.971070670242676e-15\n",
            "Epoch :  350 \tLoss -  5.517676379950551e-15\n",
            "Epoch :  351 \tLoss -  5.167002198688429e-15\n",
            "Epoch :  352 \tLoss -  5.287970362017605e-15\n",
            "Epoch :  353 \tLoss -  5.073582934935752e-15\n",
            "Epoch :  354 \tLoss -  4.877212322158472e-15\n",
            "Epoch :  355 \tLoss -  4.697173775153676e-15\n",
            "Epoch :  356 \tLoss -  4.539162319942228e-15\n",
            "Epoch :  357 \tLoss -  4.602112538032747e-15\n",
            "Epoch :  358 \tLoss -  4.557334988309096e-15\n",
            "Epoch :  359 \tLoss -  4.3755939050803185e-15\n",
            "Epoch :  360 \tLoss -  4.6975655278917814e-15\n",
            "Epoch :  361 \tLoss -  5.124274892213607e-15\n",
            "Epoch :  362 \tLoss -  5.107028030858088e-15\n",
            "Epoch :  363 \tLoss -  4.82749911145117e-15\n",
            "Epoch :  364 \tLoss -  4.484511754185381e-15\n",
            "Epoch :  365 \tLoss -  4.774915306085623e-15\n",
            "Epoch :  366 \tLoss -  4.637058999854091e-15\n",
            "Epoch :  367 \tLoss -  4.6268374297631e-15\n",
            "Epoch :  368 \tLoss -  4.4286888948295335e-15\n",
            "Epoch :  369 \tLoss -  4.253934562866183e-15\n",
            "Epoch :  370 \tLoss -  4.384281074987359e-15\n",
            "Epoch :  371 \tLoss -  4.484125507161433e-15\n",
            "Epoch :  372 \tLoss -  4.513702627130132e-15\n",
            "Epoch :  373 \tLoss -  4.30004153080055e-15\n",
            "Epoch :  374 \tLoss -  4.28397373930761e-15\n",
            "Epoch :  375 \tLoss -  4.141884809438648e-15\n",
            "Epoch :  376 \tLoss -  3.982845903262181e-15\n",
            "Epoch :  377 \tLoss -  3.850735868544822e-15\n",
            "Epoch :  378 \tLoss -  3.911117882739266e-15\n",
            "Epoch :  379 \tLoss -  3.954184849425937e-15\n",
            "Epoch :  380 \tLoss -  3.806451715512946e-15\n",
            "Epoch :  381 \tLoss -  3.635136760831922e-15\n",
            "Epoch :  382 \tLoss -  3.96592684365725e-15\n",
            "Epoch :  383 \tLoss -  4.03725379407964e-15\n",
            "Epoch :  384 \tLoss -  4.260375824913579e-15\n",
            "Epoch :  385 \tLoss -  4.2550632342684e-15\n",
            "Epoch :  386 \tLoss -  3.869525176897291e-15\n",
            "Epoch :  387 \tLoss -  3.9574103508890814e-15\n",
            "Epoch :  388 \tLoss -  3.977562958770156e-15\n",
            "Epoch :  389 \tLoss -  4.220207404972412e-15\n",
            "Epoch :  390 \tLoss -  4.0352425143463846e-15\n",
            "Epoch :  391 \tLoss -  4.122659279118344e-15\n",
            "Epoch :  392 \tLoss -  4.25915228582127e-15\n",
            "Epoch :  393 \tLoss -  4.03404862140723e-15\n",
            "Epoch :  394 \tLoss -  4.250709061403039e-15\n",
            "Epoch :  395 \tLoss -  3.98630433878582e-15\n",
            "Epoch :  396 \tLoss -  4.148146076984752e-15\n",
            "Epoch :  397 \tLoss -  3.966780652868082e-15\n",
            "Epoch :  398 \tLoss -  3.940384565132796e-15\n",
            "Epoch :  399 \tLoss -  4.116610616842001e-15\n",
            "Epoch :  400 \tLoss -  4.0442320750155945e-15\n",
            "Epoch :  401 \tLoss -  4.0596209696013106e-15\n",
            "Epoch :  402 \tLoss -  4.0739866483867435e-15\n",
            "Epoch :  403 \tLoss -  4.004924240548838e-15\n",
            "Epoch :  404 \tLoss -  4.1105975299494425e-15\n",
            "Epoch :  405 \tLoss -  3.9862179414252e-15\n",
            "Epoch :  406 \tLoss -  3.973641196224368e-15\n",
            "Epoch :  407 \tLoss -  3.894532130599079e-15\n",
            "Epoch :  408 \tLoss -  3.944961084146811e-15\n",
            "Epoch :  409 \tLoss -  3.841043270529391e-15\n",
            "Epoch :  410 \tLoss -  3.605828150307503e-15\n",
            "Epoch :  411 \tLoss -  3.738079216010579e-15\n",
            "Epoch :  412 \tLoss -  3.584136060044794e-15\n",
            "Epoch :  413 \tLoss -  3.7406948537517005e-15\n",
            "Epoch :  414 \tLoss -  4.132590316908427e-15\n",
            "Epoch :  415 \tLoss -  4.248196338165009e-15\n",
            "Epoch :  416 \tLoss -  4.366368869251772e-15\n",
            "Epoch :  417 \tLoss -  3.965973006952875e-15\n",
            "Epoch :  418 \tLoss -  4.0734254890591876e-15\n",
            "Epoch :  419 \tLoss -  3.932400856088451e-15\n",
            "Epoch :  420 \tLoss -  3.766981250720317e-15\n",
            "Epoch :  421 \tLoss -  3.5215394782097537e-15\n",
            "Epoch :  422 \tLoss -  3.1790889280742867e-15\n",
            "Epoch :  423 \tLoss -  3.286338122273258e-15\n",
            "Epoch :  424 \tLoss -  3.4902331404792347e-15\n",
            "Epoch :  425 \tLoss -  3.57356508886306e-15\n",
            "Epoch :  426 \tLoss -  3.664269188503313e-15\n",
            "Epoch :  427 \tLoss -  3.209193114294415e-15\n",
            "Epoch :  428 \tLoss -  3.086791982976722e-15\n",
            "Epoch :  429 \tLoss -  2.6592629025201913e-15\n",
            "Epoch :  430 \tLoss -  2.6577450194787116e-15\n",
            "Epoch :  431 \tLoss -  2.844962170711488e-15\n",
            "Epoch :  432 \tLoss -  2.8693783189325667e-15\n",
            "Epoch :  433 \tLoss -  2.7629708637249293e-15\n",
            "Epoch :  434 \tLoss -  2.840511647848377e-15\n",
            "Epoch :  435 \tLoss -  2.8582300947972792e-15\n",
            "Epoch :  436 \tLoss -  2.7746057082884143e-15\n",
            "Epoch :  437 \tLoss -  2.5983686451508956e-15\n",
            "Epoch :  438 \tLoss -  2.5095182771157085e-15\n",
            "Epoch :  439 \tLoss -  2.757605333520547e-15\n",
            "Epoch :  440 \tLoss -  2.9281799814056706e-15\n",
            "Epoch :  441 \tLoss -  2.593335151861837e-15\n",
            "Epoch :  442 \tLoss -  2.6524459813606887e-15\n",
            "Epoch :  443 \tLoss -  2.692290411199531e-15\n",
            "Epoch :  444 \tLoss -  2.7600447884086393e-15\n",
            "Epoch :  445 \tLoss -  2.8490061177599167e-15\n",
            "Epoch :  446 \tLoss -  2.7243664901208673e-15\n",
            "Epoch :  447 \tLoss -  2.6868965053914156e-15\n",
            "Epoch :  448 \tLoss -  2.8057684739343737e-15\n",
            "Epoch :  449 \tLoss -  2.7873084496401503e-15\n",
            "Epoch :  450 \tLoss -  2.8354756134604765e-15\n",
            "Epoch :  451 \tLoss -  2.638844538051916e-15\n",
            "Epoch :  452 \tLoss -  2.7990464204649635e-15\n",
            "Epoch :  453 \tLoss -  2.8586773281934295e-15\n",
            "Epoch :  454 \tLoss -  2.995026551053559e-15\n",
            "Epoch :  455 \tLoss -  3.0675596763928394e-15\n",
            "Epoch :  456 \tLoss -  3.2604392428780106e-15\n",
            "Epoch :  457 \tLoss -  3.0685219058209203e-15\n",
            "Epoch :  458 \tLoss -  2.9765381511556027e-15\n",
            "Epoch :  459 \tLoss -  3.1161550153680027e-15\n",
            "Epoch :  460 \tLoss -  3.055975018531479e-15\n",
            "Epoch :  461 \tLoss -  3.0140013596214764e-15\n",
            "Epoch :  462 \tLoss -  2.723652229588095e-15\n",
            "Epoch :  463 \tLoss -  2.6418451522675643e-15\n",
            "Epoch :  464 \tLoss -  2.4829796760870472e-15\n",
            "Epoch :  465 \tLoss -  2.8318759351928825e-15\n",
            "Epoch :  466 \tLoss -  2.7926062172087523e-15\n",
            "Epoch :  467 \tLoss -  2.824336706687609e-15\n",
            "Epoch :  468 \tLoss -  2.8483475496434265e-15\n",
            "Epoch :  469 \tLoss -  2.642772229828334e-15\n",
            "Epoch :  470 \tLoss -  2.5974415675901258e-15\n",
            "Epoch :  471 \tLoss -  2.6059133793703265e-15\n",
            "Epoch :  472 \tLoss -  2.4638896710383034e-15\n",
            "Epoch :  473 \tLoss -  2.4571892169090483e-15\n",
            "Epoch :  474 \tLoss -  2.3518915866205508e-15\n",
            "Epoch :  475 \tLoss -  2.6139011118211713e-15\n",
            "Epoch :  476 \tLoss -  2.7273671043365156e-15\n",
            "Epoch :  477 \tLoss -  2.663616016594368e-15\n",
            "Epoch :  478 \tLoss -  2.9254586763043794e-15\n",
            "Epoch :  479 \tLoss -  3.0207492475957777e-15\n",
            "Epoch :  480 \tLoss -  2.877059848972979e-15\n",
            "Epoch :  481 \tLoss -  2.7816151176851805e-15\n",
            "Epoch :  482 \tLoss -  2.4039034329884643e-15\n",
            "Epoch :  483 \tLoss -  2.3729820717324724e-15\n",
            "Epoch :  484 \tLoss -  2.2792093834825538e-15\n",
            "Epoch :  485 \tLoss -  2.2873732987864276e-15\n",
            "Epoch :  486 \tLoss -  2.37844628127521e-15\n",
            "Epoch :  487 \tLoss -  2.3856020156136143e-15\n",
            "Epoch :  488 \tLoss -  2.507458292987986e-15\n",
            "Epoch :  489 \tLoss -  2.3049672318920834e-15\n",
            "Epoch :  490 \tLoss -  2.4185482091300175e-15\n",
            "Epoch :  491 \tLoss -  2.4302794036912528e-15\n",
            "Epoch :  492 \tLoss -  2.2551807528348438e-15\n",
            "Epoch :  493 \tLoss -  2.4980879910089853e-15\n",
            "Epoch :  494 \tLoss -  2.556624108653125e-15\n",
            "Epoch :  495 \tLoss -  2.435633922467321e-15\n",
            "Epoch :  496 \tLoss -  2.413703180671723e-15\n",
            "Epoch :  497 \tLoss -  2.5237200550258477e-15\n",
            "Epoch :  498 \tLoss -  2.5983483163601615e-15\n",
            "Epoch :  499 \tLoss -  2.282513447251556e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBTw17uSHX3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}